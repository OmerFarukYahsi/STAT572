---
title: "hw1"
author: "Zehra Cebeci, Ömer Faruk Yahşi"
format: html
editor: visual
---

## Homework 1

## **Housing in Luxembourg**

We are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. 

![](https://raps-with-r.dev/images/lux_rhode_island.png)

In this project our goal is to:

-   Get data trapped inside an Excel file into a neat data frame;

-   Convert nominal to real prices using a simple method;

-   Make some tables and plots and call it a day (for now).

## **Saving trapped data from Excel**

The picture below shows an Excel file made for human consumption:

![](https://raps-with-r.dev/images/obs_hab_xlsx_overview.png)

So why is this file not machine-readable? Here are some issues:

-   The table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;

-   The spreadsheet starts with a header that contains an image and some text;

-   Numbers are text and use "," as the thousands separator;

-   You don't see it in the screenshot, but each year is in a separate sheet.

So first, let's load some packages:

`{r} library(dplyr) library(purrr) library(readxl) library(stringr) library(janitor)}`

Next, the code below downloads the data, and puts it in a data frame:

`{r} # The url below points to an Excel file # hosted on the book’s github repository url <- "https://is.gd/1vvBAc"  raw_data <- tempfile(fileext = ".xlsx")  download.file(url, raw_data,               method = "auto",               mode = "wb")  sheets <- excel_sheets(raw_data)  read_clean <- function(..., sheet){   read_excel(..., sheet = sheet) |>     mutate(year = sheet) }  raw_data <- map(   sheets,   ~read_clean(raw_data,               skip = 10,               sheet = .)                    ) |>   bind_rows() |>   clean_names()  raw_data <- raw_data |>   rename(     locality = commune,     n_offers = nombre_doffres,     average_price_nominal_euros = prix_moyen_annonce_en_courant,     average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,     average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant   ) |>   mutate(locality = str_trim(locality)) |>   select(year, locality, n_offers, starts_with("average"))}`

Running this code results in a neat data set:

`{r} raw_data}`

But there's a problem: columns that should be of type numeric are of type character instead (`average_price_nominal_euros` and `average_price_m2_nominal_euros`). There's also another issue, which you would eventually catch as you'll explore the data: the naming of the communes is not consistent. Let's take a look:

`{r} raw_data |>   filter(grepl("Luxembourg", locality)) |>   count(locality)}`

We can see that the city of Luxembourg is spelled in two different ways. It's the same with another commune, Pétange:

`{r} raw_data |>   filter(grepl("P.tange", locality)) |>   count(locality)}`

So sometimes it is spelled correctly, with an "é", sometimes not. Let's write some code to correct both these issues:

`{r} raw_data <- raw_data |>   mutate(     locality = ifelse(grepl("Luxembourg-Ville", locality),                       "Luxembourg",                       locality),          locality = ifelse(grepl("P.tange", locality),                            "Pétange",                            locality)          ) |>   mutate(across(starts_with("average"),          as.numeric))}`

Now this is interesting -- converting the `average` columns to numeric resulted in some `NA` values. Let's see what happened:

`{r} raw_data |>   filter(is.na(average_price_nominal_euros))}`

It turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let's go back to the raw data to see what this is about:

![](https://raps-with-r.dev/images/obs_hab_xlsx_missing.png)

So it turns out that there are some rows that we need to remove. We can start by removing rows where `locality` is missing. Then we have a row where `locality` is equal to "Total d'offres". 

Let's first remove the rows stating the sources:

`{r} raw_data <- raw_data |>   filter(!grepl("Source", locality))}`

Let's now only keep the communes in our data:

`{r} commune_level_data <- raw_data |>     filter(!grepl("nationale|offres", locality),            !is.na(locality))}`

And let's create a dataset with the national data as well:

`{r} country_level <- raw_data |>   filter(grepl("nationale", locality)) |>   select(-n_offers)  offers_country <- raw_data |>   filter(grepl("Total d.offres", locality)) |>   select(year, n_offers)  country_level_data <- full_join(country_level, offers_country) |>   select(year, locality, n_offers, everything()) |>   mutate(locality = "Grand-Duchy of Luxembourg")}`

Now the data looks clean, and we can start the actual analysis... or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. [Thankfully, Wikipedia has such a list](https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg)[6](https://raps-with-r.dev/project_start.html#fn6).

So let's scrape and save this list:

`{r} current_communes <- "https://is.gd/lux_communes" |>   rvest::read_html() |>   rvest::html_table() |>   purrr::pluck(2) |>   janitor::clean_names() |>   dplyr::filter(name_2 != "Name") |>   dplyr::rename(commune = name_2) |>   dplyr::mutate(commune = stringr::str_remove(commune, " .$"))}`

Let's see if we have all the communes in our data:

`{r} setdiff(unique(commune_level_data$locality),         current_communes$commune)}`

We see many communes that are in our `commune_level_data`, but not in `current_communes`. There's one obvious reason: differences in spelling, for example, "Kaerjeng" in our data, but "Käerjeng" in the table from Wikipedia. 

Here again, we can use a list from Wikipedia, and here again, I decide to re-host it on Github pages to avoid problems in the future:

`{r} former_communes <- "https://is.gd/lux_former_communes" |>   rvest::read_html() |>   rvest::html_table() |>   purrr::pluck(3) |>   janitor::clean_names() |>   dplyr::filter(year_dissolved > 2009)  former_communes}`

As you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:

`{r} communes <- unique(c(former_communes$name,                      current_communes$commune)) # we need to rename some communes  # Different spelling of these communes between wikipedia and the data  communes[which(communes == "Clemency")] <- "Clémency" communes[which(communes == "Redange")] <- "Redange-sur-Attert" communes[which(communes == "Erpeldange-sur-Sûre")] <- "Erpeldange" communes[which(communes == "Luxembourg City")] <- "Luxembourg" communes[which(communes == "Käerjeng")] <- "Kaerjeng" communes[which(communes == "Petange")] <- "Pétange"}`

Let's run our test again:

`{r} setdiff(unique(commune_level_data$locality),         communes)}`

Great! When we compare the communes that are in our data with every commune that has existed since 2010, we don't have any commune that is unaccounted for.

`{r} head(commune_level_data)}`

## **Analysing the data**
